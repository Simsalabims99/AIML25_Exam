{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Neural Network Classification\n",
    "\n",
    "#### Tabular Data Classification with PyTorch\n",
    "\n",
    "***\n",
    "\n",
    "Table of contents:\n",
    "\n",
    "0. Imports\n",
    "1. Setup Device\n",
    "2. Load and Prepare Data\n",
    "3. Create Dataset Class\n",
    "4. Build the Neural Network Model\n",
    "5. Create Visualization Functions\n",
    "6. Training and Evaluation Function\n",
    "7. Run the Model and show results"
   ],
   "id": "3f6870b0db27fbe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### 0. Imports",
   "id": "8c1af09eb3f7188"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:58:06.426775Z",
     "start_time": "2025-05-07T08:58:06.420652Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from helpMethods.data import to_dataloader, train_val_split\n",
    "from helpMethods.training import fit\n",
    "from helpMethods.utils import get_device"
   ],
   "id": "23511b2a87c209a7",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 1. Setup Device\n",
    "\n",
    "If available, GPU will be used to speed up training.\n"
   ],
   "id": "2447424e740c3da5"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:58:06.443193Z",
     "start_time": "2025-05-07T08:58:06.440758Z"
    }
   },
   "cell_type": "code",
   "source": "DEVICE = get_device()\n",
   "id": "961328d5fdceffb1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running pytorch version (2.5.1) with backend = mps\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 2. Load and Prepare Data\n",
    "\n",
    "We load the dataset using the shared `data_preparation` function. The function encodes target column, one-hot encodes categorical features, and checks for class imbalance. It also drops irrelevant columns and converts boolean columns to integers. The dataset is then split into features and target variable.\n"
   ],
   "id": "daa3daf921f28e30"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:58:06.467733Z",
     "start_time": "2025-05-07T08:58:06.460459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the dataset\n",
    "\n",
    "# if full dataset, uncomment the line below\n",
    "# df = pd.read_csv('Data/cleaned_data.csv')\n",
    "\n",
    "# if small sample dataset, uncomment the line below\n",
    "df = pd.read_csv('../Data/top_1000_cleaned.csv')\n",
    "\n",
    "def data_preparation(dataframe):\n",
    "    dataframe = pd.get_dummies(dataframe, columns=['Medlemstype'], drop_first=True)\n",
    "    dataframe = pd.get_dummies(dataframe, columns=['Region'], drop_first=True)\n",
    "    dataframe['Aktiv_Deltager'] = dataframe['Aktiv_Deltager'].map({'Ja': 1, 'Nej': 0})\n",
    "\n",
    "    # Balancing the dataset\n",
    "    df_majority = dataframe[dataframe['Aktiv_Deltager'] == 0]\n",
    "    df_minority = dataframe[dataframe['Aktiv_Deltager'] == 1]\n",
    "    df_minority_upsampled = df_minority.sample(len(df_majority), replace=True, random_state=42)\n",
    "    dataframe = pd.concat([df_majority, df_minority_upsampled])\n",
    "    dataframe = dataframe.sample(frac=1, random_state=42)  # Shuffle the dataset\n",
    "\n",
    "    # Splitting the data into features and target variable\n",
    "    features = dataframe.drop(columns=[\n",
    "        'Aktiv_Deltager',\n",
    "        'Kontakt_ID',\n",
    "        'Kontakt_OK',\n",
    "        'Antal_Aktiv',\n",
    "        'Status_Aarsag',\n",
    "        'Status',\n",
    "        'Startdato',\n",
    "\n",
    "        # remove below comment, optional case\n",
    "        # 'Antal'\n",
    "    ])\n",
    "    features = features.astype({col: int for col in features.select_dtypes(bool).columns})\n",
    "    target = dataframe['Aktiv_Deltager']\n",
    "\n",
    "    return features, target"
   ],
   "id": "e680b578dd3a1b6c",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 3. Create Dataset Class\n",
    "\n",
    "We define a custom PyTorch Dataset class for our tabular data. The reason for this is to allow PyTorch to handle the data in a way that is compatible with its DataLoader, which is used for batching and shuffling the data during training.\n"
   ],
   "id": "5b0dadbebe771eb"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:58:06.488551Z",
     "start_time": "2025-05-07T08:58:06.484886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TabularDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]"
   ],
   "id": "e1f8275779bc7749",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 4. Build the Neural Network Model\n",
    "\n",
    "We define a neural network with multiple fully connected layers for binary classification. The model consists of three fully connected layers with ReLU activations and dropout for regularization. The final layer outputs two classes, for whether the user is likely to sell lottery tickets or not.\n"
   ],
   "id": "d73240e01c3a8950"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:58:06.508427Z",
     "start_time": "2025-05-07T08:58:06.505449Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 64)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(32, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "id": "b365d4a0995ff854",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 5. Create visualization functions\n",
    "\n",
    "We define functions to plot the training and validation loss and accuracy curves. These plots help us visualize the model's performance over epochs, allowing us to identify potential overfitting or underfitting."
   ],
   "id": "6f45a8bc0eb225db"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_loss(history: dict):\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history[\"train_loss\"], label=\"Train Loss\")\n",
    "    plt.plot(history[\"val_loss\"], label=\"Validation Loss\")\n",
    "    plt.title(\"Loss Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(\"Visualisations/Loss_curve.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "def plot_accuracy(history: dict):\n",
    "\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history[\"train_acc\"], label=\"Train Accuracy\")\n",
    "    plt.plot(history[\"val_acc\"], label=\"Validation Accuracy\")\n",
    "    plt.title(\"Accuracy Curve\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy (%)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.savefig(\"Visualisations/Accuracy_curve.png\", dpi=300, bbox_inches='tight')"
   ],
   "id": "1346e08e960f607"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 6. Training and Evaluation Function\n",
    "\n",
    "This function handles the entire process of preparing data, training the model, evaluating and visualizing it.\n"
   ],
   "id": "e6d6ffb8dd150b2f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:58:06.522610Z",
     "start_time": "2025-05-07T08:58:06.518157Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_nn_model(data):\n",
    "    # Data preparation\n",
    "    features, target = data_preparation(data)\n",
    "\n",
    "    # Splitting the data into training, validation, and test sets\n",
    "    X_tensor = torch.FloatTensor(features.values)\n",
    "    y_tensor = torch.LongTensor(target.values)\n",
    "\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X_tensor, y_tensor, test_size=0.2, stratify=y_tensor, random_state=42\n",
    "    )\n",
    "\n",
    "    train_set = TabularDataset(X_train_val, y_train_val)\n",
    "    test_set = TabularDataset(X_test, y_test)\n",
    "    train_set, val_set = train_val_split(train_set, val_ratio=0.2, seed=42)\n",
    "\n",
    "    train_loader = to_dataloader(train_set, batch_size=64, shuffle=True)\n",
    "    val_loader = to_dataloader(val_set, batch_size=64, shuffle=False)\n",
    "    test_loader = to_dataloader(test_set, batch_size=64, shuffle=False)\n",
    "\n",
    "    model = NeuralNet(X_tensor.shape[1]).to(DEVICE)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    model, history = fit(model, train_loader, val_loader, DEVICE, optimizer, loss_fn, 10)\n",
    "\n",
    "    y_true, y_pred, y_prob = [], [], []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for features_batch, labels_batch in test_loader:\n",
    "            features_batch, labels_batch = features_batch.to(DEVICE), labels_batch.to(DEVICE)\n",
    "            out = model(features_batch)\n",
    "            _, predictions = torch.max(out, 1)\n",
    "            probabilities = torch.softmax(out, dim=1)\n",
    "            y_true.extend(labels_batch.cpu().numpy())\n",
    "            y_pred.extend(predictions.cpu().numpy())\n",
    "            y_prob.extend(probabilities[:, 1].cpu().numpy())\n",
    "\n",
    "    plot_loss(history)\n",
    "    plot_accuracy(history)\n",
    "    report = classification_report(y_true, y_pred, output_dict=True)\n",
    "    auc = roc_auc_score(y_true, y_prob) if len(set(y_true)) == 2 else None\n",
    "\n",
    "    return {\n",
    "        'accuracy': report['accuracy'],\n",
    "        'precision': report['1']['precision'],\n",
    "        'recall': report['1']['recall'],\n",
    "        'f1': report['1']['f1-score'],\n",
    "        'auc': auc\n",
    "    }"
   ],
   "id": "70b6b2b598d82c0f",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### 7. Run the Model\n",
    "\n",
    "At last, we'll run the model and display the results.\n"
   ],
   "id": "481233d3a0cb9fd7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-07T08:58:06.610155Z",
     "start_time": "2025-05-07T08:58:06.540240Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Run the model and print the results\n",
    "print(train_nn_model(df))"
   ],
   "id": "e9d23dd7eba820d3",
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2.",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Run the model and print the results\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28mprint\u001B[39m(train_nn_model(df))\n",
      "Cell \u001B[0;32mIn[21], line 9\u001B[0m, in \u001B[0;36mtrain_nn_model\u001B[0;34m(data)\u001B[0m\n\u001B[1;32m      6\u001B[0m X_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mFloatTensor(features\u001B[38;5;241m.\u001B[39mvalues)\n\u001B[1;32m      7\u001B[0m y_tensor \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mLongTensor(target\u001B[38;5;241m.\u001B[39mvalues)\n\u001B[0;32m----> 9\u001B[0m X_train_val, X_test, y_train_val, y_test \u001B[38;5;241m=\u001B[39m train_test_split(\n\u001B[1;32m     10\u001B[0m     X_tensor, y_tensor, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, stratify\u001B[38;5;241m=\u001B[39my_tensor, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m\n\u001B[1;32m     11\u001B[0m )\n\u001B[1;32m     13\u001B[0m train_set \u001B[38;5;241m=\u001B[39m TabularDataset(X_train_val, y_train_val)\n\u001B[1;32m     14\u001B[0m test_set \u001B[38;5;241m=\u001B[39m TabularDataset(X_test, y_test)\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/utils/_param_validation.py:213\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    207\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    208\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[1;32m    209\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[1;32m    210\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[1;32m    211\u001B[0m         )\n\u001B[1;32m    212\u001B[0m     ):\n\u001B[0;32m--> 213\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    215\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[1;32m    216\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[1;32m    217\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[1;32m    218\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[1;32m    219\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[1;32m    220\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    221\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    222\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[1;32m    223\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2801\u001B[0m, in \u001B[0;36mtrain_test_split\u001B[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001B[0m\n\u001B[1;32m   2797\u001B[0m         CVClass \u001B[38;5;241m=\u001B[39m ShuffleSplit\n\u001B[1;32m   2799\u001B[0m     cv \u001B[38;5;241m=\u001B[39m CVClass(test_size\u001B[38;5;241m=\u001B[39mn_test, train_size\u001B[38;5;241m=\u001B[39mn_train, random_state\u001B[38;5;241m=\u001B[39mrandom_state)\n\u001B[0;32m-> 2801\u001B[0m     train, test \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(cv\u001B[38;5;241m.\u001B[39msplit(X\u001B[38;5;241m=\u001B[39marrays[\u001B[38;5;241m0\u001B[39m], y\u001B[38;5;241m=\u001B[39mstratify))\n\u001B[1;32m   2803\u001B[0m train, test \u001B[38;5;241m=\u001B[39m ensure_common_namespace_device(arrays[\u001B[38;5;241m0\u001B[39m], train, test)\n\u001B[1;32m   2805\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mlist\u001B[39m(\n\u001B[1;32m   2806\u001B[0m     chain\u001B[38;5;241m.\u001B[39mfrom_iterable(\n\u001B[1;32m   2807\u001B[0m         (_safe_indexing(a, train), _safe_indexing(a, test)) \u001B[38;5;28;01mfor\u001B[39;00m a \u001B[38;5;129;01min\u001B[39;00m arrays\n\u001B[1;32m   2808\u001B[0m     )\n\u001B[1;32m   2809\u001B[0m )\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:1843\u001B[0m, in \u001B[0;36mBaseShuffleSplit.split\u001B[0;34m(self, X, y, groups)\u001B[0m\n\u001B[1;32m   1813\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Generate indices to split data into training and test set.\u001B[39;00m\n\u001B[1;32m   1814\u001B[0m \n\u001B[1;32m   1815\u001B[0m \u001B[38;5;124;03mParameters\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1840\u001B[0m \u001B[38;5;124;03mto an integer.\u001B[39;00m\n\u001B[1;32m   1841\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1842\u001B[0m X, y, groups \u001B[38;5;241m=\u001B[39m indexable(X, y, groups)\n\u001B[0;32m-> 1843\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m train, test \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_iter_indices(X, y, groups):\n\u001B[1;32m   1844\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m train, test\n",
      "File \u001B[0;32m~/miniconda3/lib/python3.12/site-packages/sklearn/model_selection/_split.py:2247\u001B[0m, in \u001B[0;36mStratifiedShuffleSplit._iter_indices\u001B[0;34m(self, X, y, groups)\u001B[0m\n\u001B[1;32m   2245\u001B[0m class_counts \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mbincount(y_indices)\n\u001B[1;32m   2246\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m np\u001B[38;5;241m.\u001B[39mmin(class_counts) \u001B[38;5;241m<\u001B[39m \u001B[38;5;241m2\u001B[39m:\n\u001B[0;32m-> 2247\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2248\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe least populated class in y has only 1\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2249\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m member, which is too few. The minimum\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2250\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m number of groups for any class cannot\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2251\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m be less than 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2252\u001B[0m     )\n\u001B[1;32m   2254\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m n_train \u001B[38;5;241m<\u001B[39m n_classes:\n\u001B[1;32m   2255\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m   2256\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe train_size = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m should be greater or \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   2257\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mequal to the number of classes = \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m%\u001B[39m (n_train, n_classes)\n\u001B[1;32m   2258\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: The least populated class in y has only 1 member, which is too few. The minimum number of groups for any class cannot be less than 2."
     ]
    }
   ],
   "execution_count": 22
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
